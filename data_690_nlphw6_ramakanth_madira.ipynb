{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtX2dKEhtDCO",
        "outputId": "c9aaebfa-370a-4afe-dca7-88a67b27ed35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting markovify\n",
            "  Downloading markovify-0.9.4.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting unidecode (from markovify)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: markovify\n",
            "  Building wheel for markovify (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for markovify: filename=markovify-0.9.4-py3-none-any.whl size=18608 sha256=3c1c3455c0e82e41f52e8f6ca709e4218340b3585d4f814cb4f11b1cc1158b2c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/8c/c5/41413e24c484f883a100c63ca7b3b0362b7c6f6eb6d7c9cc7f\n",
            "Successfully built markovify\n",
            "Installing collected packages: unidecode, markovify\n",
            "Successfully installed markovify-0.9.4 unidecode-1.3.8\n"
          ]
        }
      ],
      "source": [
        "!pip install markovify"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import markovify\n",
        "\n",
        "# Load data from the CSV file\n",
        "data_frame = pd.read_csv('/content/abcnews-date-text (1).csv')\n",
        "\n",
        "# Display the first three entries\n",
        "print(data_frame.head(3))\n",
        "\n",
        "# Break the text column into sentences assuming 'text' is the correct column name\n",
        "# Replace 'text' with the correct column name if different\n",
        "text_content = '.'.join(data_frame['headline_text'].tolist())\n",
        "individual_sentences = text_content.split('.')\n",
        "\n",
        "# Filter out any empty sentences\n",
        "cleaned_sentences = [sentence.strip() for sentence in individual_sentences if sentence.strip()]\n",
        "\n",
        "# Initialize a Markov chain generator\n",
        "markov_generator = markovify.Text(cleaned_sentences)\n",
        "\n",
        "# Output ten random sentences generated by the Markov model\n",
        "for j in range(10):\n",
        "    print(markov_generator.make_sentence())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOnMGXsutp6M",
        "outputId": "747f0fb2-8534-4de1-b3ed-1521c31dbb17"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   publish_date                                      headline_text\n",
            "0      20030219  aba decides against community broadcasting lic...\n",
            "1      20030219     act fire witnesses must be aware of defamation\n",
            "2      20030219     a g calls for infrastructure protection summit\n",
            "a league all stars go global\n",
            "sacked dairy workers drug use\n",
            "student initiative aims to break out\n",
            "toilet concerns for cooke\n",
            "hunter winemaker of year\n",
            "smyth continues with new threat for police pursuits\n",
            "interview tony mitchelmore gillard nasty language in three peaks race\n",
            "wheatbelt efforts help lower indigenous unemployment\n",
            "police asked to report indigenous gap widening not closing\n",
            "investigator probes cycling drug inquiry\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sumy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5eD0oIIt5CT",
        "outputId": "2eb43a46-11db-4eb4-c04a-98719c3e1d30"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/97.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/97.3 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from sumy) (2.31.0)\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-23.12.11-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from sumy) (3.8.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (4.66.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2024.2.2)\n",
            "Building wheels for collected packages: breadability, docopt\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21693 sha256=9ca3d582da727227cdb388df1c12822306e5c93f02b23697ff00da8afccf1eeb\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/22/90/b84fcc30e16598db20a0d41340616dbf9b1e82bbcc627b0b33\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=004c4da89bc06db67d331db421145f24a0cc800019b94fb51c5aadd59b0fde99\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built breadability docopt\n",
            "Installing collected packages: docopt, pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-23.12.11 sumy-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nm0E37Cmu5UC",
        "outputId": "1772783e-32cb-4836-dc9d-a76da52cd146"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "\n",
        "# Read the text file into a parser object\n",
        "text_parser = PlaintextParser.from_file(\"/content/alice.txt\", Tokenizer(\"english\"))\n",
        "\n",
        "# Create the LSA summarizer instance\n",
        "lsa_summarizer = LsaSummarizer()\n",
        "\n",
        "# Generate the summary with a specified number of sentences\n",
        "summary_result = lsa_summarizer(text_parser.document, 5)  # Adjust number of sentences as needed\n",
        "\n",
        "# Output the summarized text\n",
        "for summarized_sentence in summary_result:\n",
        "    print(summarized_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tPcgncmu8rj",
        "outputId": "4c855d24-5a99-4db5-852f-8211b4e64abd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Mouse did not answer, so Alice went on eagerly:  `There is such a nice little dog near our house I should like to show you!\n",
            "The poor little thing sobbed again (or grunted, it was impossible to say which), and they went on for some while in silence.\n",
            "He sent them word I had not gone (We know it to be true): If she should push the matter on, What would become of you?\n",
            "Don't let him know she liked them best, For this must ever be A secret, kept from all the rest, Between yourself and me.'\n",
            "`If there's no meaning in it,' said the King, `that saves a world of trouble, you know, as we needn't try to find any.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "# Define example text data\n",
        "documents = [\n",
        "    \"Climate change drives the increase in severe weather events across the globe.\",\n",
        "    \"Blockchain technology promises enhanced security in digital transactions.\",\n",
        "    \"Public health initiatives focus on preventing diseases and promoting wellness among populations.\"\n",
        "]\n",
        "\n",
        "\n",
        "# Set up the text vectorization\n",
        "text_vectorizer = TfidfVectorizer(max_df=0.95, min_df=1, stop_words='english')\n",
        "documents_tfidf = text_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Configure the NMF model\n",
        "topic_count = 3  # Number of topics to extract\n",
        "topic_model = NMF(n_components=topic_count, random_state=1)\n",
        "\n",
        "# Apply NMF to the TF-IDF matrix\n",
        "topic_model.fit(documents_tfidf)\n",
        "\n",
        "# Extract and print topics with their associated words\n",
        "word_features = text_vectorizer.get_feature_names_out()\n",
        "for topic_idx, weights in enumerate(topic_model.components_):\n",
        "    top_words = \", \".join([word_features[i] for i in weights.argsort()[-5:]])\n",
        "    print(f\"Topic {topic_idx + 1}: {top_words}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1VPXUanvNdo",
        "outputId": "71133c15-a275-4ac8-9ccf-fd041c2e3dbd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1: drives, severe, climate, change, increase\n",
            "Topic 2: digital, security, technology, transactions, blockchain\n",
            "Topic 3: promoting, public, diseases, focus, wellness\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MTHaR-U_wGLU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}